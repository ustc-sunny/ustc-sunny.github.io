# Some Good Papers about System for AI (LLM, DNN, etc)

## OSDI 25'
* [PipeThreader: Software-Defined Pipelining for Efficient DNN Execution](https://www.usenix.org/conference/osdi25/presentation/cheng)
* [Achieving Low-Latency Graph-Based Vector Search via Aligning Best-First Search Algorithm with SSD](https://www.usenix.org/conference/osdi25/presentation/guo)
* [WaferLLM: Large Language Model Inference at Wafer Scale](https://www.usenix.org/conference/osdi25/presentation/he)
* [Understanding Stragglers in Large Model Training Using What-if Analysis](https://www.usenix.org/conference/osdi25/presentation/lin-jinkun)
* [Quake: Adaptive Indexing for Vector Search](https://www.usenix.org/conference/osdi25/presentation/mohoney)
* [DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization](https://www.usenix.org/conference/osdi25/presentation/park-yeonhong)
* [Enabling Efficient GPU Communication over Multiple NICs with FuseLink](https://www.usenix.org/conference/osdi25/presentation/ren)
* [WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training](https://www.usenix.org/conference/osdi25/presentation/wang-zheng)
* [BlitzScale: Fast and Live Large Model Autoscaling with O(1) Host Caching](https://www.usenix.org/conference/osdi25/presentation/zhang-dingyan)
* [NanoFlow: Towards Optimal Large Language Model Serving Throughput](https://www.usenix.org/conference/osdi25/presentation/zhu-kan)

## ISCA 25'
* [H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference](https://dl.acm.org/doi/10.1145/3695053.3731008)
* [LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM Inference](https://dl.acm.org/doi/10.1145/3695053.3731057)
* [WSC-LLM: Efficient LLM Service and Architecture Co-exploration for Wafer-scale Chips](https://dl.acm.org/doi/10.1145/3695053.3731101)
* [AiF: Accelerating On-Device LLM Inference Using In-Flash Processing](https://dl.acm.org/doi/pdf/10.1145/3695053.3731073)
* [LIA: A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading](https://dl.acm.org/doi/pdf/10.1145/3695053.3731092)
* [DReX: Accurate and Scalable Dense Retrieval Acceleration via Algorithmic-Hardware Codesign](https://dl.acm.org/doi/full/10.1145/3695053.3731079)
* [Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware Cache Compression](https://dl.acm.org/doi/pdf/10.1145/3695053.3731024)
* [Hybe: GPU-NPU Hybrid System for Efficient LLM Inference with Million-Token Context Window](https://dl.acm.org/doi/pdf/10.1145/3695053.3731051)
* [HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation](https://dl.acm.org/doi/pdf/10.1145/3695053.3731089)
* [NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome Assembly](https://dl.acm.org/doi/pdf/10.1145/3695053.3731056)
* [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://dl.acm.org/doi/10.1145/3695053.3731116)
* [MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization](https://dl.acm.org/doi/full/10.1145/3695053.3730989)
* [WindServe: Efficient Phase-Disaggregated LLM Serving with Stream-based Dynamic Scheduling](https://dl.acm.org/doi/10.1145/3695053.3730999)
* [SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting](https://dl.acm.org/doi/pdf/10.1145/3695053.3730996)
* [Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization](https://dl.acm.org/doi/pdf/10.1145/3695053.3731019)
* [Chimera: Communication Fusion for Hybrid Parallelism in Large Language Models](https://dl.acm.org/doi/pdf/10.1145/3695053.3731025)
* [Hermes: Algorithm-System Co-design for Efficient Retrieval-Augmented Generation At-Scale](https://dl.acm.org/doi/pdf/10.1145/3695053.3731076)
* [RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://dl.acm.org/doi/10.1145/3695053.3731093)
* [Hybrid SLC-MLC RRAM Mixed-Signal Processing-in-Memory Architecture for Transformer Acceleration via Gradient Redistribution](https://dl.acm.org/doi/10.1145/3695053.3731109)
* [Topology-Aware Virtualization over Inter-Core Connected Neural Processing Units](https://dl.acm.org/doi/pdf/10.1145/3695053.3731002)
* [In-Storage Acceleration of Retrieval Augmented Generation as a Service](https://dl.acm.org/doi/pdf/10.1145/3695053.3731032)
* [Debunking the CUDA Myth Towards GPU-based AI Systems: Evaluation of the Performance and Programmability of Intel’s Gaudi NPU for AI Model Serving](https://dl.acm.org/doi/10.1145/3695053.3731050)
* [MagiCache: A Virtual In-Cache Computing Engine](https://dl.acm.org/doi/pdf/10.1145/3695053.3731113)
* [Folded Banks: 3D-Stacked HBM Design for Fine-Grained Random-Access Bandwidth](https://dl.acm.org/doi/pdf/10.1145/3695053.3731111)
* [BingoGCN: Towards Scalable and Efficient GNN Acceleration with Fine-Grained Partitioning and SLT](https://dl.acm.org/doi/pdf/10.1145/3695053.3731115)

## ATC 25'
* [Jenga: Enhancing LLM Long-Context Fine-tuning with Contextual Token Sparsity](https://www.usenix.org/system/files/atc25-wang-tuowei.pdf)
* [Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation](https://www.usenix.org/system/files/atc25-feng.pdf)
* [WEAVER: Efficient Multi-LLM Serving with Attention Offloading](https://www.usenix.org/system/files/atc25-gao.pdf)
* [CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the Edge](https://www.usenix.org/system/files/atc25-tian.pdf)
* [TOPPINGS: CPU-Assisted, Rank-Aware Adapter Serving for LLM Inference](https://www.usenix.org/system/files/atc25-li-suyi-toppings.pdf)
* [AssyLLM: Efficient Federated Fine-tuning of LLMs via Assembling Pre-trained Blocks](https://www.usenix.org/system/files/atc25-zhan.pdf)
* [DeepServe: Serverless Large Language Model Serving at Scale](https://www.usenix.org/system/files/atc25-hu-junhao.pdf)
* [GeneralSparse: Bridging the Gap in SpMM for Pruned Large Language Model Inference on GPUs](https://www.usenix.org/system/files/atc25-wang-yaoyu.pdf)
* [QFactory: Accelerating Quantized Large Language Model Serving with Qtile Graphs](https://www.usenix.org/system/files/atc25-zhang-qihao.pdf)
* [Obscura: Concealing Recomputation Overhead in Training of Large Language Models with Bubble-filling Pipeline Transformation](https://www.usenix.org/system/files/atc25-huang-yuzhou.pdf)
* [Resource Multiplexing in Tuning and Serving Large Language Models](https://www.usenix.org/system/files/atc25-he-yongjun.pdf)
* [KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provide](https://www.usenix.org/system/files/atc25-wang-jiahao.pdf)
* [LogCrisp: Fast Aggregated Analysis on Large-scale Compressed Logs by Enabling Two-Phase Pattern Extraction and Vectorized Queries](https://www.usenix.org/system/files/atc25-wei.pdf)
* [SNARY: A High-Performance and Generic SmartNIC-accelerated Retrieval System](https://www.usenix.org/system/files/atc25-gan.pdf)

## HPCA 25'
* [BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration](https://arxiv.org/abs/2411.11745)
* [MANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](https://arxiv.org/abs/2502.18755)
* [DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency](https://arxiv.org/abs/2408.00741)
* [throttLL’eM: Predictive GPU Throttling for Energy Efficient LLM Inference Serving](https://ieeexplore.ieee.org/document/10946751)
* [Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format](https://arxiv.org/abs/2411.15982)
* [LAD: Efficient Accelerator for Generative Inference of LLM with Locality Aware Decoding](https://ieeexplore.ieee.org/abstract/document/10946807/)
* [VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference](https://ieeexplore.ieee.org/document/10946800/)
* [InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://ieeexplore.ieee.org/document/10946721)
* [PAISE: PIM-Accelerated Inference Scheduling Engine for Transformer-based LLM](https://ieeexplore.ieee.org/document/10946299/)
* [FACIL: Flexible DRAM Address Mapping for SoC-PIM Cooperative On-device LLM Inference](https://ieeexplore.ieee.org/abstract/document/10946824)
* [Lincoln: Real-Time 50~100B LLM Inference on Consumer Devices with LPDDR-Interfaced, Compute-Enabled Flash Memory](https://ieeexplore.ieee.org/document/10946816)
* [Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM](https://ieeexplore.ieee.org/document/10946712/)
* [eDKM: An Efficient and Accurate Train-Time Weight Clustering for Large Language Models](https://ieeexplore.ieee.org/document/10946750)
* [NeuVSA: A Unified and Efficient Accelerator for Neural Vector Search](https://ieeexplore.ieee.org/document/10946742/)
