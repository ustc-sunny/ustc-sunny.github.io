# Some Good Papers about System for AI (LLM, DNN, etc)

## OSDI 2025
* [PipeThreader: Software-Defined Pipelining for Efficient DNN Execution](https://www.usenix.org/conference/osdi25/presentation/cheng)
* [Achieving Low-Latency Graph-Based Vector Search via Aligning Best-First Search Algorithm with SSD](https://www.usenix.org/conference/osdi25/presentation/guo)
* [WaferLLM: Large Language Model Inference at Wafer Scale](https://www.usenix.org/conference/osdi25/presentation/he)
* [Understanding Stragglers in Large Model Training Using What-if Analysis](https://www.usenix.org/conference/osdi25/presentation/lin-jinkun)
* [Quake: Adaptive Indexing for Vector Search](https://www.usenix.org/conference/osdi25/presentation/mohoney)
* [DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization](https://www.usenix.org/conference/osdi25/presentation/park-yeonhong)
* [Enabling Efficient GPU Communication over Multiple NICs with FuseLink](https://www.usenix.org/conference/osdi25/presentation/ren)
* [WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training](https://www.usenix.org/conference/osdi25/presentation/wang-zheng)
* [BlitzScale: Fast and Live Large Model Autoscaling with O(1) Host Caching](https://www.usenix.org/conference/osdi25/presentation/zhang-dingyan)
* [NanoFlow: Towards Optimal Large Language Model Serving Throughput](https://www.usenix.org/conference/osdi25/presentation/zhu-kan)

## ISCA 2025
* [H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference](https://dl.acm.org/doi/10.1145/3695053.3731008)
* [LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM Inference](https://dl.acm.org/doi/10.1145/3695053.3731057)
* [WSC-LLM: Efficient LLM Service and Architecture Co-exploration for Wafer-scale Chips](https://dl.acm.org/doi/10.1145/3695053.3731101)
* [AiF: Accelerating On-Device LLM Inference Using In-Flash Processing](https://dl.acm.org/doi/pdf/10.1145/3695053.3731073)
* [LIA: A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading](https://dl.acm.org/doi/pdf/10.1145/3695053.3731092)
* [DReX: Accurate and Scalable Dense Retrieval Acceleration via Algorithmic-Hardware Codesign](https://dl.acm.org/doi/full/10.1145/3695053.3731079)
* [Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware Cache Compression](https://dl.acm.org/doi/pdf/10.1145/3695053.3731024)
* [Hybe: GPU-NPU Hybrid System for Efficient LLM Inference with Million-Token Context Window](https://dl.acm.org/doi/pdf/10.1145/3695053.3731051)
* [HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation](https://dl.acm.org/doi/pdf/10.1145/3695053.3731089)
* [NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome Assembly](https://dl.acm.org/doi/pdf/10.1145/3695053.3731056)
* [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://dl.acm.org/doi/10.1145/3695053.3731116)
* [MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization](https://dl.acm.org/doi/full/10.1145/3695053.3730989)
* [WindServe: Efficient Phase-Disaggregated LLM Serving with Stream-based Dynamic Scheduling](https://dl.acm.org/doi/10.1145/3695053.3730999)
* [SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting](https://dl.acm.org/doi/pdf/10.1145/3695053.3730996)
* [Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization](https://dl.acm.org/doi/pdf/10.1145/3695053.3731019)
* [Chimera: Communication Fusion for Hybrid Parallelism in Large Language Models](https://dl.acm.org/doi/pdf/10.1145/3695053.3731025)
* [Hermes: Algorithm-System Co-design for Efficient Retrieval-Augmented Generation At-Scale](https://dl.acm.org/doi/pdf/10.1145/3695053.3731076)
* [RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://dl.acm.org/doi/10.1145/3695053.3731093)
* [Hybrid SLC-MLC RRAM Mixed-Signal Processing-in-Memory Architecture for Transformer Acceleration via Gradient Redistribution](https://dl.acm.org/doi/10.1145/3695053.3731109)
* [Topology-Aware Virtualization over Inter-Core Connected Neural Processing Units](https://dl.acm.org/doi/pdf/10.1145/3695053.3731002)
* [In-Storage Acceleration of Retrieval Augmented Generation as a Service](https://dl.acm.org/doi/pdf/10.1145/3695053.3731032)
* [Debunking the CUDA Myth Towards GPU-based AI Systems: Evaluation of the Performance and Programmability of Intelâ€™s Gaudi NPU for AI Model Serving](https://dl.acm.org/doi/10.1145/3695053.3731050)
* [MagiCache: A Virtual In-Cache Computing Engine](https://dl.acm.org/doi/pdf/10.1145/3695053.3731113)
* [Folded Banks: 3D-Stacked HBM Design for Fine-Grained Random-Access Bandwidth](https://dl.acm.org/doi/pdf/10.1145/3695053.3731111)
* [BingoGCN: Towards Scalable and Efficient GNN Acceleration with Fine-Grained Partitioning and SLT](https://dl.acm.org/doi/pdf/10.1145/3695053.3731115)
